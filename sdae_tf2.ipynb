{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Denoising Autoencoder regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, GaussianNoise, LeakyReLU, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential, load_model, save_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1708641289864651689\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4930941747\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7906018761298007651\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#Checking whether your machine running only on CPU or with GPU\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape : (86, 15063)\n",
      "no. of features :  15063\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"flesh_color_gene.txt\", delim_whitespace=True, header=None,)\n",
    "dataset = dataframe.values\n",
    "print(\"data shape :\", dataframe.shape)\n",
    "data_len = dataset.shape[1]\n",
    "print(\"no. of features : \", data_len )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into taining and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX :  (68, 15062)\n",
      "trainY :  (68,)\n",
      "testX :  (18, 15062)\n",
      "testY :  (18,)\n"
     ]
    }
   ],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "train, test = train_test_split(dataset, test_size=0.2)\n",
    "trainX = train[:,0:data_len-1]\n",
    "trainY = train[:,data_len-1]\n",
    "testX = test[:,0:data_len-1]\n",
    "testY = test[:,data_len-1]\n",
    "\n",
    "print(\"trainX : \", trainX.shape)\n",
    "print(\"trainY : \", trainY.shape)\n",
    "print(\"testX : \", testX.shape)\n",
    "print(\"testY : \", testY.shape)\n",
    "#np.savetxt('Y.txt', trainY)\n",
    "#np.savetxt('Y_test.txt', testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normX shape :  (68, 15062)\n",
      "X norm shape : (18, 15062)\n",
      "small train :  (68, 8000)\n",
      "smnall test :  (18, 8000)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing train data\n",
    "\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "normX = max_abs_scaler.fit_transform(trainX)\n",
    "input_shape = normX.shape\n",
    "#print(\"X norm : \", normX[:,15439])\n",
    "print(\"normX shape : \", input_shape)\n",
    "#np.savetxt('xnorm.txt', normX)\n",
    "small_train = normX[:, 0:8000]\n",
    "\n",
    "# Normalizing test data\n",
    "\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "normXt = max_abs_scaler.fit_transform(testX)\n",
    "#np.savetxt('test_set.txt', normXt)\n",
    "print(\"X norm shape :\", normXt.shape)\n",
    "small_test = normXt[:, 0:8000]\n",
    "\n",
    "print(\"small train : \", small_train.shape)\n",
    "print(\"smnall test : \", small_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='loss', patience=50)\n",
    "#K.set_learning_phase(1)\n",
    "checkpointer = ModelCheckpoint(filepath='flesh_color.h5', verbose=1, save_best_only=True)\n",
    "input_dims = data_len-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Simple Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPULSORY!!! run the function below before go to the Stacked Denoising Autoencoder\n",
    "\n",
    "Since my GPU can support only up to 6GB of VRAM (NVIDIA GTX 1060ti), I set only run on GPU if the features is less than 10,000. You can set it based on your machine capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dae (inputX, input_dims, output_dims, epoch, activation, loss, opti):\n",
    "    \n",
    "    #input_dims = inputX.shape\n",
    "    #print(\"input dims : \", input_dims)\n",
    "        \n",
    "    model = Sequential()\n",
    "    \n",
    "    if input_dims > 9999:\n",
    "        with tensorflow.device('/cpu:0'):\n",
    "            print(\"Using CPU....\")\n",
    "            #model.add(Dense(input_dims, input_dim = input_dims))\n",
    "            #model.add(GaussianNoise(0.5), input_shape=(input_dims, ))\n",
    "            model.add(Dense(input_dims, input_dim = input_dims))\n",
    "            model.add(GaussianNoise(0.5))\n",
    "            model.add(Dense(output_dims, activation= activation, kernel_regularizer = regularizers.l1(0.01)))\n",
    "            model.add(Dense(input_dims, activation= activation))\n",
    "            model.compile(loss = loss, optimizer = opti)\n",
    "            model.fit(inputX, inputX, epochs = epoch, batch_size = 16)\n",
    "            model.summary()\n",
    "    else:\n",
    "        with tensorflow.device('/device:GPU:0'):\n",
    "            print(\"Using GPU....\")\n",
    "            #model.add(GaussianNoise(0.5), input_shape=(input_dims, ))\n",
    "            model.add(Dense(input_dims, input_dim = input_dims))\n",
    "            model.add(GaussianNoise(0.5))\n",
    "            model.add(Dense(output_dims, activation= activation, kernel_regularizer = regularizers.l1(0.01)))\n",
    "            model.add(Dense(input_dims, activation= activation))\n",
    "            model.compile(loss = loss, optimizer = opti)\n",
    "            model.fit(inputX, inputX, epochs = epoch, batch_size = 16)\n",
    "            model.summary()\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need to run the cells below if you want to go straight to the Stacked Denoising Autoencoder. This code is to show how the Denoising Autoencoder works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU....\n",
      "Train on 68 samples\n",
      "Epoch 1/3\n",
      "68/68 [==============================] - 1s 10ms/sample - loss: 525.1592\n",
      "Epoch 2/3\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 417.7400\n",
      "Epoch 3/3\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 352.9378\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8000)              64008000  \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               4000500   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8000)              4008000   \n",
      "=================================================================\n",
      "Total params: 72,016,500\n",
      "Trainable params: 72,016,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = dae(small_train, \n",
    "                  input_dims = 8000, \n",
    "                  output_dims = 500, \n",
    "                  epoch = 3, \n",
    "                  activation = 'relu', \n",
    "                  loss = 'mse', \n",
    "                  opti = 'adam',\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8000)              64008000  \n",
      "_________________________________________________________________\n",
      "gaussian_noise (GaussianNois (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               4000500   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8000)              4008000   \n",
      "=================================================================\n",
      "Total params: 72,016,500\n",
      "Trainable params: 72,016,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Stacked Denoising Autoencoder - pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter\n",
    "\n",
    "layers = [8000, 5000, 3000, 1000, 500, 300, 100] # Setting the size of your layer\n",
    "epoch = 3\n",
    "optimizer = 'adamax'\n",
    "activation = 'relu'\n",
    "loss = 'mse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdae_pretrain (inputX, layers, activation, epoch, optimizer, loss):\n",
    "    \n",
    "    encoder = []\n",
    "    decoder = []\n",
    "    ae = []\n",
    "    \n",
    "    for i in range(len(layers)-1):\n",
    "            # Greedily train each layer\n",
    "            print(\"Now pretraining layer {} [{}-->{}]\".format(i+1, layers[i], layers[i+1]))\n",
    "\n",
    "            input_dims = layers[i]\n",
    "            output_dims = layers[i+1]\n",
    "            \n",
    "            autoencoder = dae(inputX, input_dims, output_dims, epoch, activation, loss, optimizer)\n",
    "            \n",
    "            enc = Sequential()\n",
    "            enc.add(Dense(output_dims, input_dim=input_dims))\n",
    "            enc.compile(loss=loss, optimizer=optimizer)\n",
    "            enc.layers[0].set_weights(autoencoder.layers[2].get_weights())\n",
    "            inputX = enc.predict(inputX)\n",
    "            print(\"check dimension : \", inputX.shape)\n",
    "            enc.summary()\n",
    "            #print(i)\n",
    "            encoder.append(autoencoder.layers[2].get_weights())\n",
    "            decoder.append(autoencoder.layers[3].get_weights())\n",
    "            ae.append(autoencoder)\n",
    "\n",
    "    \n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now pretraining layer 1 [8000-->5000]\n",
      "Using GPU....\n",
      "Train on 68 samples\n",
      "Epoch 1/3\n",
      "68/68 [==============================] - 1s 13ms/sample - loss: 3911.8391\n",
      "Epoch 2/3\n",
      "68/68 [==============================] - 0s 5ms/sample - loss: 2990.0804\n",
      "Epoch 3/3\n",
      "68/68 [==============================] - 0s 5ms/sample - loss: 2408.30780s - loss: 2516.59 - ETA: 0s - loss: 2467.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 8000)              64008000  \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5000)              40005000  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8000)              40008000  \n",
      "=================================================================\n",
      "Total params: 144,021,000\n",
      "Trainable params: 144,021,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "check dimension :  (68, 5000)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 5000)              40005000  \n",
      "=================================================================\n",
      "Total params: 40,005,000\n",
      "Trainable params: 40,005,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Now pretraining layer 2 [5000-->3000]\n",
      "Using GPU....\n",
      "Train on 68 samples\n",
      "Epoch 1/3\n",
      "68/68 [==============================] - 1s 8ms/sample - loss: 1826.3866\n",
      "Epoch 2/3\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 1223.7497\n",
      "Epoch 3/3\n",
      "68/68 [==============================] - 0s 2ms/sample - loss: 763.3865\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 5000)              25005000  \n",
      "_________________________________________________________________\n",
      "gaussian_noise_2 (GaussianNo (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3000)              15003000  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5000)              15005000  \n",
      "=================================================================\n",
      "Total params: 55,013,000\n",
      "Trainable params: 55,013,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "check dimension :  (68, 3000)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 3000)              15003000  \n",
      "=================================================================\n",
      "Total params: 15,003,000\n",
      "Trainable params: 15,003,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Now pretraining layer 3 [3000-->1000]\n",
      "Using GPU....\n",
      "Train on 68 samples\n",
      "Epoch 1/3\n",
      "68/68 [==============================] - 0s 5ms/sample - loss: 534.0450\n",
      "Epoch 2/3\n",
      "68/68 [==============================] - 0s 662us/sample - loss: 403.4065\n",
      "Epoch 3/3\n",
      "68/68 [==============================] - 0s 632us/sample - loss: 293.6224\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 3000)              9003000   \n",
      "_________________________________________________________________\n",
      "gaussian_noise_3 (GaussianNo (None, 3000)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1000)              3001000   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3000)              3003000   \n",
      "=================================================================\n",
      "Total params: 15,007,000\n",
      "Trainable params: 15,007,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "check dimension :  (68, 1000)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 1000)              3001000   \n",
      "=================================================================\n",
      "Total params: 3,001,000\n",
      "Trainable params: 3,001,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Now pretraining layer 4 [1000-->500]\n",
      "Using GPU....\n",
      "Train on 68 samples\n",
      "Epoch 1/3\n",
      "68/68 [==============================] - 0s 6ms/sample - loss: 150.1138\n",
      "Epoch 2/3\n",
      "68/68 [==============================] - 0s 279us/sample - loss: 127.1343\n",
      "Epoch 3/3\n",
      "68/68 [==============================] - 0s 265us/sample - loss: 106.2731\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "gaussian_noise_4 (GaussianNo (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1000)              501000    \n",
      "=================================================================\n",
      "Total params: 2,002,500\n",
      "Trainable params: 2,002,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "check dimension :  (68, 500)\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 500)               500500    \n",
      "=================================================================\n",
      "Total params: 500,500\n",
      "Trainable params: 500,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Now pretraining layer 5 [500-->300]\n",
      "Using GPU....\n",
      "Train on 68 samples\n",
      "Epoch 1/3\n",
      "68/68 [==============================] - 0s 4ms/sample - loss: 62.7628\n",
      "Epoch 2/3\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 55.7287\n",
      "Epoch 3/3\n",
      "68/68 [==============================] - 0s 221us/sample - loss: 49.1686\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "gaussian_noise_5 (GaussianNo (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 500)               150500    \n",
      "=================================================================\n",
      "Total params: 551,300\n",
      "Trainable params: 551,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "check dimension :  (68, 300)\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 300)               150300    \n",
      "=================================================================\n",
      "Total params: 150,300\n",
      "Trainable params: 150,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Now pretraining layer 6 [300-->100]\n",
      "Using GPU....\n",
      "Train on 68 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 4ms/sample - loss: 18.0575\n",
      "Epoch 2/3\n",
      "68/68 [==============================] - 0s 191us/sample - loss: 16.6350\n",
      "Epoch 3/3\n",
      "68/68 [==============================] - 0s 206us/sample - loss: 15.2901\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "gaussian_noise_6 (GaussianNo (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 300)               30300     \n",
      "=================================================================\n",
      "Total params: 150,700\n",
      "Trainable params: 150,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "check dimension :  (68, 100)\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 100)               30100     \n",
      "=================================================================\n",
      "Total params: 30,100\n",
      "Trainable params: 30,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test = sdae_pretrain(small_train, layers = layers, activation = activation, epoch = epoch, optimizer = optimizer, loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: Saving the models for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pre_train_l10.hd5\\assets\n",
      "INFO:tensorflow:Assets written to: pre_train_l11.hd5\\assets\n",
      "INFO:tensorflow:Assets written to: pre_train_l12.hd5\\assets\n",
      "INFO:tensorflow:Assets written to: pre_train_l13.hd5\\assets\n",
      "INFO:tensorflow:Assets written to: pre_train_l14.hd5\\assets\n",
      "INFO:tensorflow:Assets written to: pre_train_l15.hd5\\assets\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(test):\n",
    "    filename=\"pre_train_l1\" + str(i) + \".hd5\"\n",
    "    m.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Stacked Denoising Autoencoder - fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: Load pre-trained model if you have one. Skip these 2 cells below if dont have one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "with tensorflow.device('/cpu:0'):\n",
    "    for i in range (len(layers)-1):\n",
    "    \n",
    "        test.append(load_model(\"pre_train_l1\"+ str(i) + \".hd5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning the Autoencoders with the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(weights, inputX, inputXt, inputY, inputYt, layers, epoch, activation, batch, optimizer, loss):\n",
    "\n",
    "    encoder = []\n",
    "    decoder = []\n",
    "\n",
    "    for i in range(len(test)):\n",
    "    \n",
    "        encoder.append(test[i].layers[2].get_weights())\n",
    "        decoder.append(test[i].layers[3].get_weights())\n",
    "    \n",
    "    with tensorflow.device('/device:gpu:0'): #I have to put this because the model size is to big for my GPU\n",
    "        ft = Sequential()\n",
    "        ft.add(Dense(layers[0], input_dim=layers[0]))\n",
    "        ft.add(GaussianNoise(0.5))\n",
    "\n",
    "        for i in range(len(layers)-1):\n",
    "        #print(i)\n",
    "            ft.add(Dense(layers[i+1], activation = activation, weights = encoder[i], kernel_regularizer = regularizers.l1_l2(0.01)))\n",
    "        \n",
    "        for i in reversed(range(len(layers)-1)):\n",
    "        #print(i)\n",
    "            ft.add(Dense(layers[i], activation = activation, weights = decoder[i]))\n",
    "    ft.add(Dropout(0.2))\n",
    "    ft.add(Dense(1000, activation = activation))\n",
    "    ft.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    #ft.add(Dense(1, kernel_initializer = 'normal'))   \n",
    "    ft.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    ft.fit(small_train, trainY, epochs = epoch, batch_size = batch, validation_data=(small_test, testY))\n",
    "    ft.summary()\n",
    "    \n",
    "    return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-aa4d4315dcc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtryla\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfine_tuning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmall_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmall_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-e7c077bf5f99>\u001b[0m in \u001b[0;36mfine_tuning\u001b[1;34m(weights, inputX, inputXt, inputY, inputYt, layers, epoch, activation, batch, optimizer, loss)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mget_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1322\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m         \u001b[0moutput_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1324\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_updates_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m   3268\u001b[0m   \"\"\"\n\u001b[0;32m   3269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3270\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3271\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3272\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3268\u001b[0m   \"\"\"\n\u001b[0;32m   3269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3270\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3271\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3272\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    580\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    584\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    636\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msparse_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   3824\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3825\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3826\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3827\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3828\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6605\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6606\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6607\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity]"
     ]
    }
   ],
   "source": [
    "tryla = fine_tuning(test, small_train, small_test, trainY, testY, layers = layers, epoch = 10, activation = 'relu', batch = 16, optimizer = 'adam', loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryla.predict(x=normXt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryla.save(\"fine_tune.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
