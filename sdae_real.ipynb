{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Denoising Autoencoder regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faizm\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Input, GaussianNoise, LeakyReLU\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape : (86, 15441)\n",
      "no. of features :  15441\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"flesh_color.txt\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "print(\"data shape :\", dataframe.shape)\n",
    "data_len = dataset.shape[1]\n",
    "print(\"no. of features : \", data_len )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into taining and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX :  (68, 15440)\n",
      "trainY :  (68,)\n",
      "testX :  (18, 15440)\n",
      "testY :  (18,)\n"
     ]
    }
   ],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "train, test = train_test_split(dataset, test_size=0.2)\n",
    "trainX = train[:,0:data_len-1]\n",
    "trainY = train[:,data_len-1]\n",
    "testX = test[:,0:data_len-1]\n",
    "testY = test[:,data_len-1]\n",
    "\n",
    "print(\"trainX : \", trainX.shape)\n",
    "print(\"trainY : \", trainY.shape)\n",
    "print(\"testX : \", testX.shape)\n",
    "print(\"testY : \", testY.shape)\n",
    "#np.savetxt('Y.txt', trainY)\n",
    "#np.savetxt('Y_test.txt', testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normX shape :  (68, 15440)\n",
      "X norm shape : (18, 15440)\n",
      "small train :  (68, 8000)\n",
      "smnall test :  (18, 8000)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing train data\n",
    "\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "normX = max_abs_scaler.fit_transform(trainX)\n",
    "input_shape = normX.shape\n",
    "#print(\"X norm : \", normX[:,15439])\n",
    "print(\"normX shape : \", input_shape)\n",
    "#np.savetxt('xnorm.txt', normX)\n",
    "small_train = normX[:, 0:8000]\n",
    "\n",
    "# Normalizing test data\n",
    "\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "normXt = max_abs_scaler.fit_transform(testX)\n",
    "#np.savetxt('test_set.txt', normXt)\n",
    "print(\"X norm shape :\", normXt.shape)\n",
    "small_test = normXt[:, 0:8000]\n",
    "\n",
    "print(\"small train : \", small_train.shape)\n",
    "print(\"smnall test : \", small_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='loss', patience=50)\n",
    "K.set_learning_phase(1)\n",
    "checkpointer = ModelCheckpoint(filepath='flesh_color.h5', verbose=1, save_best_only=True)\n",
    "input_dims = data_len-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Simple Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPULSORY!!! run the function below before go to the Stacked Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dae (inputX, input_dims, output_dims, epoch, activation, loss, opti):\n",
    "    \n",
    "    #input_dims = inputX.shape\n",
    "    #print(\"input dims : \", input_dims)\n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "    #session = tf.Session(config=config)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(input_dims, input_dim = input_dims))\n",
    "    model.add(GaussianNoise(0.5))\n",
    "    model.add(Dense(output_dims, activation= activation, kernel_regularizer = regularizers.l1(0.01)))\n",
    "    model.add(Dense(input_dims, activation= activation))\n",
    "    \n",
    "    model.compile(loss = loss, optimizer = opti)\n",
    "    model.fit(inputX, inputX, epochs = epoch, batch_size = 4)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need to run code below if you want to go straight to the Stacked Denoising Autoencoder. This code is to show how the Denoising Autoencoder works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "autoencoder = dae(normX, input_dims =15440, output_dims = 10000, epoch = 3, activation = 'relu', loss = 'mse', opti = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Stacked Denoising Autoencoder - pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter\n",
    "\n",
    "layers = [input_dims, 10000, 5000, 3000, 1000, 500, 300, 100]\n",
    "epoch = 3\n",
    "optimizer = 'adamax'\n",
    "activation = 'relu'\n",
    "loss = 'mse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdae_pretrain (inputX, layers, activation, epoch, optimizer, loss):\n",
    "    \n",
    "    encoder = []\n",
    "    decoder = []\n",
    "    ae = []\n",
    "    \n",
    "    for i in range(len(layers)-1):\n",
    "            # Greedily train each layer\n",
    "            print(\"Now pretraining layer {} [{}-->{}]\".format(i+1, layers[i], layers[i+1]))\n",
    "\n",
    "            input_dims = layers[i]\n",
    "            output_dims = layers[i+1]\n",
    "            \n",
    "            autoencoder = dae(inputX, input_dims, output_dims, epoch, activation, loss, optimizer)\n",
    "            \n",
    "            enc = Sequential()\n",
    "            enc.add(Dense(output_dims, input_dim=input_dims))\n",
    "            enc.compile(loss=loss, optimizer=optimizer)\n",
    "            enc.layers[0].set_weights(autoencoder.layers[2].get_weights())\n",
    "            inputX = enc.predict(inputX)\n",
    "            print(\"check dimension : \", inputX.shape)\n",
    "            enc.summary()\n",
    "            #print(i)\n",
    "            encoder.append(autoencoder.layers[2].get_weights())\n",
    "            decoder.append(autoencoder.layers[3].get_weights())\n",
    "            ae.append(autoencoder)\n",
    "\n",
    "    \n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = sdae_pretrain(normX, layers = layers, activation = activation, epoch = epoch, optimizer = optimizer, loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: Saving the models for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    \n",
    "    test[i].save(\"pre_train_l1\" + str(i) + \".hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Stacked Denoising Autoencoder - fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "for i in range (len(layers)-1):\n",
    "    \n",
    "    test.append(load_model(\"pre_train_l1\"+ str(i) + \".hd5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(weights, inputX, inputXt, inputY, inputYt, layers, epoch, activation, batch, optimizer, loss):\n",
    "\n",
    "    encoder = []\n",
    "    decoder = []\n",
    "\n",
    "    for i in range(len(test)):\n",
    "    \n",
    "        encoder.append(test[i].layers[2].get_weights())\n",
    "        decoder.append(test[i].layers[3].get_weights())\n",
    "    \n",
    "    \n",
    "    ft = Sequential()\n",
    "    ft.add(Dense(layers[0], input_dim=layers[0]))\n",
    "    ft.add(GaussianNoise(0.5))\n",
    "\n",
    "    for i in range(len(layers)-1):\n",
    "        #print(i)\n",
    "        ft.add(Dense(layers[i+1], activation = activation, weights = encoder[i], kernel_regularizer = regularizers.l1_l2(0.01)))\n",
    "        \n",
    "    for i in reversed(range(len(layers)-1)):\n",
    "        #print(i)\n",
    "        ft.add(Dense(layers[i], activation = activation, weights = decoder[i]))\n",
    "    \n",
    "    #ft.add(Dense(1, kernel_initializer = 'normal'))   \n",
    "    ft.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    ft.fit(normX, normX, epochs = epoch, batch_size = batch, validation_data=(inputXt, inputXt))\n",
    "    ft.summary()\n",
    "    \n",
    "    return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryla = fine_tuning(test, normX, normXt, trainY, testY, layers = layers, epoch = 1000, activation = 'relu', batch = 16, optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryla.predict(x=normXt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryla.save(\"fine_tune.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
