{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Denoising Autoencoder regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Input, GaussianNoise, LeakyReLU\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4184927994315154264\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4951913267\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16846310861428380862\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape : (194, 5001)\n",
      "no. of features :  5001\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_excel(\"dna_methy.xlsx\", delim_whitespace=True, header=None, skiprows=1)\n",
    "dataset = dataframe.values\n",
    "print(\"data shape :\", dataframe.shape)\n",
    "data_len = dataset.shape[1]\n",
    "print(\"no. of features : \", data_len )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into taining and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX :  (155, 5000)\n",
      "testX :  (39, 5000)\n"
     ]
    }
   ],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "train, test = train_test_split(dataset, test_size=0.2)\n",
    "trainX = train[:,1:data_len]\n",
    "#trainY = train[:,data_len-1]\n",
    "testX = test[:,1:data_len]\n",
    "#testY = test[:,data_len-1]\n",
    "\n",
    "print(\"trainX : \", trainX.shape)\n",
    "#print(\"trainY : \", trainY.shape)\n",
    "print(\"testX : \", testX.shape)\n",
    "#print(\"testY : \", testY.shape)\n",
    "#np.savetxt('Y.txt', trainY)\n",
    "#np.savetxt('Y_test.txt', testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normX shape :  (155, 5000)\n",
      "X norm shape : (39, 5000)\n",
      "small train :  (155, 5000)\n",
      "smnall test :  (39, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing train data\n",
    "\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "normX = max_abs_scaler.fit_transform(trainX)\n",
    "input_shape = normX.shape\n",
    "#print(\"X norm : \", normX[:,15439])\n",
    "print(\"normX shape : \", input_shape)\n",
    "#np.savetxt('xnorm.txt', normX)\n",
    "small_train = normX[:, 0:8000]\n",
    "\n",
    "# Normalizing test data\n",
    "\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "normXt = max_abs_scaler.fit_transform(testX)\n",
    "#np.savetxt('test_set.txt', normXt)\n",
    "print(\"X norm shape :\", normXt.shape)\n",
    "small_test = normXt[:, 0:8000]\n",
    "\n",
    "print(\"small train : \", small_train.shape)\n",
    "print(\"smnall test : \", small_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='loss', patience=50)\n",
    "checkpointer = ModelCheckpoint(filepath='test.h5', verbose=1, save_best_only=True)\n",
    "input_dims = data_len-1\n",
    "lr_decay = ReduceLROnPlateau(monitor='val_loss', mode = 'min',factor=0.5, patience=5, min_lr=0.0001, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Simple Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPULSORY!!! run the function below before go to the Stacked Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dae (inputX, input_dims, output_dims, epoch, activation, loss, opti):\n",
    "    \n",
    "    #input_dims = inputX.shape\n",
    "    #print(\"input dims : \", input_dims)\n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "    #session = tf.Session(config=config)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(input_dims, input_dim = input_dims))\n",
    "    model.add(GaussianNoise(0.5))\n",
    "    model.add(Dense(output_dims, activation= activation, kernel_regularizer = regularizers.l1(0.01)))\n",
    "    model.add(Dense(input_dims, activation= activation))\n",
    "    \n",
    "    model.compile(loss = loss, optimizer = opti)\n",
    "    model.summary()\n",
    "    model.fit(inputX, inputX, epochs = epoch, batch_size = 4, callbacks = [checkpointer, lr_decay, TQDMNotebookCallback()], verbose =0)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need to run code below if you want to go straight to the Stacked Denoising Autoencoder. This code is to show how the Denoising Autoencoder works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 5000)              25005000  \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5000)              25005000  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5000)              25005000  \n",
      "=================================================================\n",
      "Total params: 75,015,000\n",
      "Trainable params: 75,015,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebb0b14eb1b4d9eaf8547be56f08f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faizm\\Anaconda3\\envs\\faiz-gpu\\lib\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\faizm\\Anaconda3\\envs\\faiz-gpu\\lib\\site-packages\\keras\\callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "autoencoder = dae(normX, \n",
    "                  input_dims = input_dims, \n",
    "                  output_dims = input_dims, \n",
    "                  epoch = 3, \n",
    "                  activation = 'relu', \n",
    "                  loss = 'mse', \n",
    "                  opti = 'adam',\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Stacked Denoising Autoencoder - pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter\n",
    "\n",
    "layers = [input_dims, 3000, 1000, 500, 300, 100]\n",
    "epoch = 3\n",
    "optimizer = 'adamax'\n",
    "activation = 'relu'\n",
    "loss = 'mse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdae_pretrain (inputX, layers, activation, epoch, optimizer, loss):\n",
    "    \n",
    "    encoder = []\n",
    "    decoder = []\n",
    "    ae = []\n",
    "    \n",
    "    for i in range(len(layers)-1):\n",
    "            # Greedily train each layer\n",
    "            print(\"Now pretraining layer {} [{}-->{}]\".format(i+1, layers[i], layers[i+1]))\n",
    "\n",
    "            input_dims = layers[i]\n",
    "            output_dims = layers[i+1]\n",
    "            \n",
    "            autoencoder = dae(inputX, input_dims, output_dims, epoch, activation, loss, optimizer)\n",
    "            \n",
    "            enc = Sequential()\n",
    "            enc.add(Dense(output_dims, input_dim=input_dims))\n",
    "            enc.compile(loss=loss, optimizer=optimizer)\n",
    "            enc.layers[0].set_weights(autoencoder.layers[2].get_weights())\n",
    "            inputX = enc.predict(inputX)\n",
    "            print(\"check dimension : \", inputX.shape)\n",
    "            enc.summary()\n",
    "            #print(i)\n",
    "            encoder.append(autoencoder.layers[2].get_weights())\n",
    "            decoder.append(autoencoder.layers[3].get_weights())\n",
    "            ae.append(autoencoder)\n",
    "\n",
    "    \n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now pretraining layer 1 [5000-->3000]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 5000)              25005000  \n",
      "_________________________________________________________________\n",
      "gaussian_noise_2 (GaussianNo (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3000)              15003000  \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5000)              15005000  \n",
      "=================================================================\n",
      "Total params: 55,013,000\n",
      "Trainable params: 55,013,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318aa16daee7470baaa98702462daedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faizm\\Anaconda3\\envs\\faiz-gpu\\lib\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "C:\\Users\\faizm\\Anaconda3\\envs\\faiz-gpu\\lib\\site-packages\\keras\\callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "check dimension :  (155, 3000)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 3000)              15003000  \n",
      "=================================================================\n",
      "Total params: 15,003,000\n",
      "Trainable params: 15,003,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Now pretraining layer 2 [3000-->1000]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 3000)              9003000   \n",
      "_________________________________________________________________\n",
      "gaussian_noise_3 (GaussianNo (None, 3000)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1000)              3001000   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3000)              3003000   \n",
      "=================================================================\n",
      "Total params: 15,007,000\n",
      "Trainable params: 15,007,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c3b05760214ce79cc62c70a3b00639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "check dimension :  (155, 1000)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 1000)              3001000   \n",
      "=================================================================\n",
      "Total params: 3,001,000\n",
      "Trainable params: 3,001,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Now pretraining layer 3 [1000-->500]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "gaussian_noise_4 (GaussianNo (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1000)              501000    \n",
      "=================================================================\n",
      "Total params: 2,002,500\n",
      "Trainable params: 2,002,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78644c114d1497b91520c93c58d438e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db7f905a257495fbf3924a1d67e7131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=155), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = sdae_pretrain(normX, layers = layers, activation = activation, epoch = epoch, optimizer = optimizer, loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: Saving the models for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    \n",
    "    test[i].save(\"pre_train_l1\" + str(i) + \".hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Stacked Denoising Autoencoder - fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "for i in range (len(layers)-1):\n",
    "    \n",
    "    test.append(load_model(\"pre_train_l1\"+ str(i) + \".hd5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(weights, inputX, inputXt, inputY, inputYt, layers, epoch, activation, batch, optimizer, loss):\n",
    "\n",
    "    encoder = []\n",
    "    decoder = []\n",
    "\n",
    "    for i in range(len(test)):\n",
    "    \n",
    "        encoder.append(test[i].layers[2].get_weights())\n",
    "        decoder.append(test[i].layers[3].get_weights())\n",
    "    \n",
    "    \n",
    "    ft = Sequential()\n",
    "    ft.add(Dense(layers[0], input_dim=layers[0]))\n",
    "    ft.add(GaussianNoise(0.5))\n",
    "\n",
    "    for i in range(len(layers)-1):\n",
    "        #print(i)\n",
    "        ft.add(Dense(layers[i+1], activation = activation, weights = encoder[i], kernel_regularizer = regularizers.l1_l2(0.01)))\n",
    "        \n",
    "    for i in reversed(range(len(layers)-1)):\n",
    "        #print(i)\n",
    "        ft.add(Dense(layers[i], activation = activation, weights = decoder[i]))\n",
    "    \n",
    "    #ft.add(Dense(1, kernel_initializer = 'normal'))   \n",
    "    ft.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    ft.fit(normX, normX, epochs = epoch, batch_size = batch, validation_data=(inputXt, inputXt))\n",
    "    ft.summary()\n",
    "    \n",
    "    return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryla = fine_tuning(test, normX, normXt, trainY, testY, layers = layers, epoch = 1000, activation = 'relu', batch = 16, optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryla.predict(x=normXt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryla.save(\"fine_tune.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
