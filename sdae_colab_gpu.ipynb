{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Denoising Autoencoder regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Input, GaussianNoise, LeakyReLU\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/content/gdrive/My Drive/experiment/datasets/' ##Change this according to your path drive\n",
    "# load dataset\n",
    "dataframe = pandas.read_excel(data_path + \"dna_methy.xlsx\", delim_whitespace=True, header=None, skiprows=1)\n",
    "dataset = dataframe.values\n",
    "print(\"data shape :\", dataframe.shape)\n",
    "data_len = dataset.shape[1]\n",
    "print(\"no. of features : \", data_len )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into taining and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "train, test = train_test_split(dataset, test_size=0.2)\n",
    "trainX = train[:,1:data_len]\n",
    "#trainY = train[:,data_len-1]\n",
    "testX = test[:,1:data_len]\n",
    "#testY = test[:,data_len-1]\n",
    "\n",
    "print(\"trainX : \", trainX.shape)\n",
    "#print(\"trainY : \", trainY.shape)\n",
    "print(\"testX : \", testX.shape)\n",
    "#print(\"testY : \", testY.shape)\n",
    "#np.savetxt('Y.txt', trainY)\n",
    "#np.savetxt('Y_test.txt', testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing train data\n",
    "\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "normX = max_abs_scaler.fit_transform(trainX)\n",
    "input_shape = normX.shape\n",
    "#print(\"X norm : \", normX[:,15439])\n",
    "print(\"normX shape : \", input_shape)\n",
    "#np.savetxt('xnorm.txt', normX)\n",
    "small_train = normX[:, 0:8000]\n",
    "\n",
    "# Normalizing test data\n",
    "\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "normXt = max_abs_scaler.fit_transform(testX)\n",
    "#np.savetxt('test_set.txt', normXt)\n",
    "print(\"X norm shape :\", normXt.shape)\n",
    "small_test = normXt[:, 0:8000]\n",
    "\n",
    "print(\"small train : \", small_train.shape)\n",
    "print(\"smnall test : \", small_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='loss', patience=50)\n",
    "checkpointer = ModelCheckpoint(filepath='test.h5', verbose=1, save_best_only=True)\n",
    "input_dims = data_len-1\n",
    "lr_decay = ReduceLROnPlateau(monitor='val_loss', mode = 'min',factor=0.5, patience=5, min_lr=0.0001, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Simple Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPULSORY!!! run the function below before go to the Stacked Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dae (inputX, input_dims, output_dims, epoch, activation, loss, opti):\n",
    "    \n",
    "    #input_dims = inputX.shape\n",
    "    #print(\"input dims : \", input_dims)\n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "    #session = tf.Session(config=config)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(input_dims, input_dim = input_dims))\n",
    "    model.add(GaussianNoise(0.5))\n",
    "    model.add(Dense(output_dims, activation= activation, kernel_regularizer = regularizers.l1(0.01)))\n",
    "    model.add(Dense(input_dims, activation= activation))\n",
    "    \n",
    "    model.compile(loss = loss, optimizer = opti)\n",
    "    model.summary()\n",
    "    model.fit(inputX, inputX, epochs = epoch, batch_size = 4, callbacks = [checkpointer, lr_decay, TQDMNotebookCallback()], verbose =0)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't need to run code below if you want to go straight to the Stacked Denoising Autoencoder. This code is to show how the Denoising Autoencoder works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autoencoder = dae(normX, \n",
    "                  input_dims = input_dims, \n",
    "                  output_dims = input_dims, \n",
    "                  epoch = 3, \n",
    "                  activation = 'relu', \n",
    "                  loss = 'mse', \n",
    "                  opti = 'adam',\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Stacked Denoising Autoencoder - pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter\n",
    "\n",
    "layers = [input_dims, 3000, 1000, 500, 300, 100]\n",
    "epoch = 3\n",
    "optimizer = 'adamax'\n",
    "activation = 'relu'\n",
    "loss = 'mse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdae_pretrain (inputX, layers, activation, epoch, optimizer, loss):\n",
    "    \n",
    "    encoder = []\n",
    "    decoder = []\n",
    "    ae = []\n",
    "    \n",
    "    for i in range(len(layers)-1):\n",
    "            # Greedily train each layer\n",
    "            print(\"Now pretraining layer {} [{}-->{}]\".format(i+1, layers[i], layers[i+1]))\n",
    "\n",
    "            input_dims = layers[i]\n",
    "            output_dims = layers[i+1]\n",
    "            \n",
    "            autoencoder = dae(inputX, input_dims, output_dims, epoch, activation, loss, optimizer)\n",
    "            \n",
    "            enc = Sequential()\n",
    "            enc.add(Dense(output_dims, input_dim=input_dims))\n",
    "            enc.compile(loss=loss, optimizer=optimizer)\n",
    "            enc.layers[0].set_weights(autoencoder.layers[2].get_weights())\n",
    "            inputX = enc.predict(inputX)\n",
    "            print(\"check dimension : \", inputX.shape)\n",
    "            enc.summary()\n",
    "            #print(i)\n",
    "            encoder.append(autoencoder.layers[2].get_weights())\n",
    "            decoder.append(autoencoder.layers[3].get_weights())\n",
    "            ae.append(autoencoder)\n",
    "\n",
    "    \n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = sdae_pretrain(normX, layers = layers, activation = activation, epoch = epoch, optimizer = optimizer, loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: Saving the models for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    \n",
    "    test[i].save(\"pre_train_l1\" + str(i) + \".hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Stacked Denoising Autoencoder - fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "for i in range (len(layers)-1):\n",
    "    \n",
    "    test.append(load_model(\"pre_train_l1\"+ str(i) + \".hd5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(weights, inputX, inputXt, inputY, inputYt, layers, epoch, activation, batch, optimizer, loss):\n",
    "\n",
    "    encoder = []\n",
    "    decoder = []\n",
    "\n",
    "    for i in range(len(test)):\n",
    "    \n",
    "        encoder.append(test[i].layers[2].get_weights())\n",
    "        decoder.append(test[i].layers[3].get_weights())\n",
    "    \n",
    "    \n",
    "    ft = Sequential()\n",
    "    ft.add(Dense(layers[0], input_dim=layers[0]))\n",
    "    ft.add(GaussianNoise(0.5))\n",
    "\n",
    "    for i in range(len(layers)-1):\n",
    "        #print(i)\n",
    "        ft.add(Dense(layers[i+1], activation = activation, weights = encoder[i], kernel_regularizer = regularizers.l1_l2(0.01)))\n",
    "        \n",
    "    for i in reversed(range(len(layers)-1)):\n",
    "        #print(i)\n",
    "        ft.add(Dense(layers[i], activation = activation, weights = decoder[i]))\n",
    "    ft.add(Dropout(0.2))\n",
    "    ft.add(Dense(1000, activation = activation))\n",
    "    ft.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    #ft.add(Dense(1, kernel_initializer = 'normal'))   \n",
    "    ft.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    ft.fit(normX, normX, epochs = epoch, batch_size = batch, validation_data=(inputXt, inputXt))\n",
    "    ft.summary()\n",
    "    \n",
    "    return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryla = fine_tuning(test, normX, normXt, trainY, testY, layers = layers, epoch = 1000, activation = 'relu', batch = 16, optimizer = 'adam', loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryla.predict(x=normXt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryla.save(\"fine_tune.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
